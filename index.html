<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />

    <!-- BEGIN Info -->
    <meta
      name="description"
      content="Guardrail - An open-source tool that generates regression tests for microservices using recorded production traffic."
    />
    <meta name="title" property="og:title" content="Guardrail" />
    <meta property="og:type" content="Website" />

    <meta name="image" property="og:image" content="images/thumb.png" />

    <meta
      name="description"
      property="og:description"
      content="Guardrail - An open-source tool that generates regression tests for microservices using recorded production traffic."
    />
    <meta name="author" content="Guardrail" />
    <!-- END Info -->

    <!-- BEGIN favicon -->
    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="images/favicon/apple-touch-icon.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="images/favicon/favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="images/favicon/favicon-16x16.png"
    />
    <link rel="manifest" href="images/favicon/site.webmanifest" />
    <link
      rel="mask-icon"
      href="images/favicon/safari-pinned-tab.svg"
      color="#5bbad5"
    />
    <link rel="shortcut icon" href="images/favicon/favicon.ico" />
    <meta name="msapplication-TileColor" content="#da532c" />
    <meta
      name="msapplication-config"
      content="images/favicon/browserconfig.xml"
    />
    <meta name="theme-color" content="#ffffff" />
    <!-- END favicon -->

    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Guardrail</title>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css"
    />
    <link
      rel="stylesheet"
      href="https://unpkg.com/@tailwindcss/typography@0.2.x/dist/typography.min.css"
    />
    <link rel="stylesheet" href="stylesheets/reset.css" />
    <link rel="stylesheet" href="stylesheets/style.css" />
    <link rel="stylesheet" href="stylesheets/responsive.css" />
  </head>

  <body>
    <header class="mobile-menu-closed">
      <div id="header">
        <a href="/">
          <img src="images/logo/logo-name.svg" />
        </a>
        <nav>
          <a href="#start-here" class="selected">Start Here</a>
          <a href="#case-study">Case Study</a>
          <a href="#presentation">Presentation</a>
          <a href="#our-team">Our Team</a>

          <a
            href="https://github.com/guardrail-service-testing"
            target="_blank"
            class="icon"
            ><i class="fab fa-github"></i
          ></a>
        </nav>
        <div id="menu">
          <button type="button">
            <svg
              id="mobile-open"
              xmlns="http://www.w3.org/2000/svg"
              fill="none"
              viewBox="0 0 24 24"
              stroke="currentColor"
              aria-hidden="true"
            >
              <path
                stroke-linecap="round"
                stroke-linejoin="round"
                stroke-width="2"
                d="M4 6h16M4 12h16M4 18h16"
              />
            </svg>
            <svg
              id="mobile-close"
              xmlns="http://www.w3.org/2000/svg"
              fill="none"
              viewBox="0 0 24 24"
              stroke="currentColor"
              aria-hidden="true"
            >
              <path
                stroke-linecap="round"
                stroke-linejoin="round"
                stroke-width="2"
                d="M6 18L18 6M6 6l12 12"
              />
            </svg>
          </button>
        </div>
      </div>

      <div id="header-buffer"></div>

      <div id="mobile-menu">
        <a href="#start-here" class="selected">Start Here</a>
        <a href="#case-study">Case Study</a>
        <a href="#presentation">Presentation</a>
        <a href="#our-team">Our Team</a>

        <a href="https://github.com/guardrail-service-testing" target="_blank"
          ><i class="fab fa-github"></i> GitHub</a
        >
      </div>
    </header>

    <div id="start-here" class="main-section">
      <div class="h-full">
        <div class="static-logo-color"></div>
        <div class="bg-blue">
          <img
            class="guardrail sm-screen"
            src="images/logo/guardrail-logo-name-mono-on-dark-bg.png"
          />
          <img
            class="guardrail lg-screen"
            src="images/logo/guardrail-text-caps.png"
          />

          <p class="light-text">
            An open-source tool that generates <br />
            <span class="text-teal">regression tests</span> for microservices
            using<br />
            <span class="text-orange">recorded</span> production traffic.<br />
          </p>
        </div>
      </div>
      <div class="h-full">
        <div class="bg-teal static-logo-teal-light">
          <h2>Easy to Manage & Deploy</h2>
        </div>
        <div class="bg-teal">
          <h2 class="sm-header">Easy to Manage & Deploy</h2>
          <p>
            Lorem ipsum dolor sit amet consectetur adipisicing elit. Odio minima
            libero corporis dolor dolorum quam beatae architecto fugit
            praesentium.
          </p>
          <img
            src="https://via.placeholder.com/350x150.gif"
            alt="placeholder"
          />
        </div>
      </div>
      <div class="h-full">
        <div class="bg-gainsboro static-logo-grey-light">
          <h2>Modular and Flexible</h2>
        </div>
        <div class="bg-blue">
          <h2 class="sm-header">Modular and Flexible</h2>
          <p>
            Lorem, ipsum dolor sit amet consectetur adipisicing elit. Debitis
            quia suscipit iusto quos quidem deleniti?
          </p>
          <img
            src="https://via.placeholder.com/350x150.gif"
            alt="placeholder"
          />
        </div>
      </div>
    </div>

    <aside id="toc">
      <ul>
        <li data-section="section-1" class="selected">
          <a href="#section-1">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Introduction</p>
            </div>
          </a>
        </li>
        <li data-section="section-2">
          <a href="#section-2">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Testing Microservices</p>
            </div>
          </a>
        </li>
        <li data-section="section-2" class="subitem">
          <a href="#section-2-1">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Testing in Production vs. Outside of Production</p>
            </div>
          </a>
        </li>
        <li data-section="section-2" class="subitem">
          <a href="#section-2-2">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Regression Testing without Recorded Production Traffic</p>
            </div>
          </a>
        </li>
        <li data-section="section-2" class="subitem">
          <a href="#section-2-3">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Regression Testing with Recorded Production Traffic</p>
            </div>
          </a>
        </li>
        <li data-section="section-3">
          <a href="#section-3">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Why We Built Guardrail</p>
            </div>
          </a>
        </li>
        <li data-section="section-4">
          <a href="#section-4">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>What is Guardrail</p>
            </div>
          </a>
        </li>
        <li data-section="section-5">
          <a href="#section-5">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Using Guardrail</p>
            </div>
          </a>
        </li>
        <li data-section="section-5" class="subitem">
          <a href="#section-5-1">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Record</p>
            </div>
          </a>
        </li>
        <li data-section="section-5" class="subitem">
          <a href="#section-5-2">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Replay</p>
            </div>
          </a>
        </li>
        <li data-section="section-5" class="subitem">
          <a href="#section-5-3">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Report Results</p>
            </div>
          </a>
        </li>
        <li data-section="section-6">
          <a href="#section-6">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Design Decisions</p>
            </div>
          </a>
        </li>
        <li data-section="section-6" class="subitem">
          <a href="#section-6-1">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Inserting Upstream Recording Instrumentation</p>
            </div>
          </a>
        </li>
        <li data-section="section-6" class="subitem">
          <a href="#section-6-2">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Inserting Downstream Recording Instrumentation</p>
            </div>
          </a>
        </li>
        <li data-section="section-6" class="subitem">
          <a href="#section-6-3">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Coordinating Upstream and Downstream Recording</p>
            </div>
          </a>
        </li>
        <li data-section="section-6" class="subitem">
          <a href="#section-6-4">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Correlating Upstream Traffic with Downstream Traffic</p>
            </div>
          </a>
        </li>
        <li data-section="section-6" class="subitem">
          <a href="#section-6-5">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Collecting Data in One Place</p>
            </div>
          </a>
        </li>
        <li data-section="section-7">
          <a href="#section-7">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Future Work</p>
            </div>
          </a>
        </li>
        <li data-section="section-7" class="subitem">
          <a href="#section-7-1">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Encryption</p>
            </div>
          </a>
        </li>
        <li data-section="section-7" class="subitem">
          <a href="#section-7-2">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Diffing Outbound Requests</p>
            </div>
          </a>
        </li>
        <li data-section="section-7" class="subitem">
          <a href="#section-7-3">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Toggle Simulating Response Times</p>
            </div>
          </a>
        </li>
        <li data-section="section-7" class="subitem">
          <a href="#section-7-4">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Filtering Requests</p>
            </div>
          </a>
        </li>
        <li data-section="section-7" class="subitem">
          <a href="#section-7-5">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Comparison Middleware</p>
            </div>
          </a>
        </li>
        <li data-section="section-8">
          <a href="#section-8">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Conclusion</p>
            </div>
          </a>
        </li>
        <li data-section="section-9">
          <a href="#section-9">
            <div>
              <div class="bullet">
                <div></div>
              </div>
              <p>Bibliography</p>
            </div>
          </a>
        </li>
      </ul>
    </aside>

    <div id="case-study" class="main-section">
      <div id="case-study-content">
        <div class="prose">
          <h1>Case Study</h1>
          <h2 id="section-1">1. Introduction</h2>
          <p>
            Guardrail is an open-source “traffic replay” tool created to help
            reduce incidents in production by using recorded HTTP traffic to
            generate regression tests for stateless microservices. The tests run
            as out-of-process component tests using traffic replay and service
            virtualization.
          </p>
          <p>
            This case study explains the engineering problem Guardrail
            addresses, how it works, and some of the key technical challenges we
            encountered while building it. Before we get into those details, we
            want to explain the problem that we sought to address.
          </p>
          <h2 id="section-2">2. Testing Microservices</h2>
          <p>
            To understand Guardrail, let’s first talk about why developers want
            to test microservices.
          </p>
          <p>
            We’ll base our conversation on the following hypothetical scenario.
            Aaron, a web developer, is responsible for maintaining an online
            store with the following service-oriented architecture.
          </p>
          <img class="lazy" data-src="images/diagrams/store.drawio.png" />
          <p>
            As you can see, in this architecture, requests from users first hit
            an API gateway, which then issues HTTP requests to the “store
            service,” which in turn issues its own HTTP requests to the
            “shipping service.”
          </p>
          <p>
            As the maintainer of the app, Aaron decides to make a change to the
            implementation of the “store service.” They run their unit tests on
            the changes they’ve made to the code, and then they deploy those
            changes into the production environment.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/store_update.drawio.png"
          />
          <p>
            However, there is also the possibility that the updated code, once
            deployed, introduces unforeseen changes into the microservice as a
            whole, and the service starts to mishandle traffic in the production
            environment. If the application has low fault tolerance, mishandled
            requests could lead to other services failing and the entire system
            crashing. According to the 2019 State of DevOps Report (DevOps
            Research and Assessment), 15% of deployed changes cause production
            incidents that could lead to service impairment or service outage.
            Subsequently, they require remediation.
          </p>
          <img class="lazy" data-src="images/diagrams/store_error.drawio.png" />
          <p>
            From this scenario, we know that Aaron needs to confirm that the
            <em>new</em> version of the microservice
            <em>as a whole</em> operates as expected. He needs to verify the
            quality of the deployed changes.
          </p>
          <h3 id="section-2-1">
            2.1 Testing in Production vs. Outside of Production
          </h3>
          <p>
            Now, Aaron needs to decide how to go about that test. There are
            broadly two categories for testing microservices: those run in
            production, which means the new code is deployed and tested using
            real traffic, and those done outside of the production environment,
            often using manually scripted traffic. Both types deal with
            recreating the environment and data.
          </p>
          <h4 id="section-2-1-1">2.1.1 Testing in Production</h4>
          <p>
            Testing in production aims to surface problems that cannot be
            detected in the pre-production environment (Shridharan). It gives
            confidence that a problem caught on a smaller scale won’t cause
            issues if fully deployed. Of course, this is not the only test that
            should be relied on.
          </p>
          <p>
            Testing in production takes many forms. One of the most common forms
            is canary deployment. If Aaron were to release the new version of
            the store service using a canary deployment, he would leave the
            original version of the “store service” running in production. Then,
            he would deploy an instance of the updated “store service” and then
            have a load balancer to split traffic meant for the “store service”
            between the two versions.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/store_canary.drawio.png"
          />
          <p>
            Aaron would then use his production environment’s error-monitoring
            tooling to assess the new “store service”’s functionality. If he
            doesn’t see a significant increase in errors, he can be reasonably
            confident that he can swap out the old version with the new version.
            Suppose he does see an increase in errors. In that case, he can
            reconfigure the API gateway to stop sending traffic to the “store
            service 2.0,” and the application will continue to operate with
            minimal interruption.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/store_canary_error.drawio.png"
          />
          <p>
            This approach improves Aaron’s previous course by limiting the
            “blast radius” caused by potential problems. Now, if the modified
            “store service” mishandles traffic, it will affect a small portion
            of production traffic instead of 100% of the traffic, and it will be
            easier for Aaron to roll back the changes he made. Google’s “Site
            Reliability Engineering&quot; gives an example - “If we instead use
            a canary population of 5%, we serve 20% errors for 5% of traffic,
            resulting in a 1% overall error rate” (Google, Inc.).
          </p>
          <p>
            Netflix employees Andy Glover and Katharina Probst share their
            experience with this approach: “Whenever you deploy a new version of
            your app, there are two things to keep in mind: first, are you
            (and/or your colleagues) able to watch the impacts of the deployment
            and available to remediate if need be? And second,
            <em>should</em> there be a problem with your rollout, are you
            limiting the blast radius to the fewest customers possible?” (Glover
            and Probst) Though effective, this approach requires careful
            planning and tooling. There needs to be an initial investment in
            observability to detect problems, and there needs to be strategies
            for rapid rollback. Testing in production alone, even with these
            valuable tools, is not perfect.
          </p>
          <p>
            There are cases where mishandled production traffic could have
            significant repercussions, even if it’s a tiny portion of overall
            traffic. In highly regulated software industries like banking or
            medical record applications, a canary deployment mishandling a few
            requests could put a company in non-compliance. Or, even in Aaron’s
            e-commerce application, a few mishandled requests could result in
            the application overcharging some customers, and the business might
            not be willing to risk that happening.
          </p>
          <p>
            Going forward, let’s say this is the case for Aaron, and his
            business cannot tolerate the risks involved with relying solely on
            testing in production. How can he make sure that the “store service”
            will behave as expected once he changes it? He needs to invest more
            in pre-production tests.
          </p>
          <h4 id="section-2-1-2">2.1.2 Testing Outside of Production</h4>
          <p>
            To figure out what the pre-production test would look like, let’s
            think about what Aaron is trying to do: He wants to replace an old
            version of a microservice with a newer version of that same service.
          </p>
          <p>
            This means that we want to prove that any given request, when issued
            to the original version of the microservice or the new version of
            the microservice, will result in the same response. If that is the
            case, we can say that the recent changes haven’t broken anything.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/regression_test.drawio.png"
          />
          <p>
            This is called a <strong>regression test</strong>. We are not making
            changes to the e-commerce application such that the traffic handled
            by the “Store Service” changes. We assume that the new, updated
            version of the service will receive very similar traffic to the old
            version being replaced. Suppose the responses of the service under
            test and the deployed version for any given request have the same
            HTTP headers and body. In that case, the application should not be
            affected by replacing the old version with the new version.
          </p>
          <p>
            Of course, there are many different ways a microservice can cause
            errors in production and many ways to catch those potential failures
            using tests (Toby Clemenson). Guardrail is exclusively focused on
            catching errors pertaining to the contents of HTTP responses.
          </p>
          <p>
            Testing outside of production introduces the need for additional
            tooling - now that we are outside of production, we have to set up
            the testing environment such that it closely mimics production. In
            contrast, when testing in production, the service under test lives
            in the same environment as its “test data” and service dependencies.
            There’s no need to recreate the API requests and data. There’s no
            need to replicate the production environment.
          </p>
          <h3 id="section-2-2">
            2.2 Regression Testing Without Traffic Replay
          </h3>
          <p>
            We have established that a) we need to test the functionality of the
            “Store Service 2.0” as a whole, b) we need to run those tests
            outside of the production environment, and c) regression tests are a
            way to do that.
          </p>
          <p>
            There are two challenges developers encounter when creating and
            running these regression tests:
          </p>
          <ol>
            <li>Generating requests to issue against the service under test</li>
            <li>
              Making downstream services available to the service under test
            </li>
          </ol>
          <h4 id="section-2-2-1">2.2.1 Generating Requests</h4>
          <p>
            One common approach to this challenge is for the developer to
            manually script the requests based on their understanding of the
            business logic and API. In other words, the developer comes up with
            a set of HTTP requests to issue against the service under test and
            the response they expect for each request.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/script_test_setup.drawio.png"
          />
          <p>
            They then issue those requests against an instance of the
            microservice that they have spun up in a testing environment and
            compare the actual responses received from the service under test
            with the expected responses.
          </p>
          <img class="lazy" data-src="images/diagrams/run_tests.drawio.png" />
          <p>
            This approach may be adequate for applications that have simple
            APIs. However, the traffic a microservice is handling may be
            unpredictable. It is difficult for a developer to manually script
            requests that represent traffic the service will need to handle in
            production. It&#39;s not enough to script a single request, and you
            may need to script a sequence of requests to emulate a user&#39;s
            transaction. The developer needs to anticipate actual usage
            patterns. Not only that, he then needs to provide realistic test
            data.
          </p>
          <p>
            On top of that, the tests become stale when the new API rolls out.
            Old tests need to be removed, and new ones have to be created.
            Manually writing and maintaining those tests is one of the
            challenges of regression tests.
          </p>
          <h4 id="section-2-2-2">
            2.2.2 Making Downstream Dependencies Available
          </h4>
          <p>
            The second challenge when creating regression tests is making
            downstream dependencies available to the service under test.
          </p>
          <p>
            Recall the architecture of Aaron’s e-commerce application. The
            “store service” needs to issue requests to a “shipping service” to
            handle the traffic it receives from the API Gateway. In this case,
            the “shipping service” is a “downstream dependency” of the “store
            service.”
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/downstream_dependency.drawio.png"
          />
          <p>
            Downstream dependencies can be internal to an application, as is the
            case in Aaron’s e-commerce store, or they can be external to the
            application.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/test_third_party.drawio.png"
            alt="Example of a downstream dependency that is external to an
          application"
          />
          <p>
            The approach to this challenge depends on whether the downstream
            dependency is internal to the application or hosted by a third
            party. If the downstream dependency is internal to the application,
            as is the case with Aaron’s e-commerce application, then it can be
            spun up in the same environment as the service under test.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/test_dependency.drawio.png"
          />
          <p>
            This solution may work with a few simple downstream dependencies.
            If, for instance, they are easily contained in a docker container,
            it won’t take too much effort to spin up those dependencies in the
            testing environment.
          </p>
          <p>
            However, it’s not practical for architectures with multiple, complex
            dependencies.
          </p>
          <p>
            In terms of complexity, a dependency could be a SaaS application,
            which can’t always be run in a container for testing.
          </p>
          <p>
            In terms of the number of dependencies, spinning up many
            microservices in the same testing environment makes that environment
            brittle. Many factors can cause problems, and it could be the
            integration settings, replication of configuration, or other things.
            “If your test needs to deploy a large number of services, there’s a
            good chance that one of them will fail to deploy, making tests
            unreliable.” (Richardson Section 10.3).
          </p>
          <p>
            The other concern with spinning up many dependency services is that
            it requires teams to coordinate. One of the main advantages of
            having a distributed architecture is allowing teams to advance
            independently, but requiring them to correspond to run tests reduces
            each team’s independence.
          </p>
          <p>
            If the dependency is external to the application’s architecture,
            meaning a third party operates it, Aaron cannot spin it up locally.
            Instead, he can interact with the third-party dependency from his
            test environment. This only works if the third-party offers a
            testing API, is not rate-limited, or the effects will be
            inconsequential (for example, won’t charge anyone or send real
            emails).
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/test_third_party_2.drawio.png"
          />
          <p>
            The other challenge of testing with a third-party dependency is
            determinism. This solution will not work if the responses change
            over time. For example, suppose the dependency service during the
            test returns the current exchange rate of USD to euros. In that
            case, the data used by the service under tests will change every
            time the developer runs a suite of tests, making it impossible to
            predict what the expected response should be for that suite of
            tests.
          </p>
          <p>
            Altogether, the challenges in testing a microservice before
            releasing it to production are complex for these reasons: How does a
            developer know what requests to issue against it? And how do they
            create an isolated testing environment without having to spin up
            multiple, complicated services? <strong>Traffic replay</strong> is a
            testing pattern intended to solve this problem.
          </p>
          <h3 id="section-2-3">2.3 Regression Testing with Traffic Replay</h3>
          <p>
            Let’s revisit the same two challenges addressed previously and how
            traffic replay can be used to make microservice testing more
            manageable and more robust.
          </p>
          <h4 id="section-2-3-1">
            2.3.1 Generating Requests using Traffic Replay
          </h4>
          <p>
            “Traffic replay” uses recorded production traffic as the set of both
            requests and expected responses. They are requests to issue against
            the service under test and the expected response for each of those
            requests. That is, Aaron adds instrumentation to the production
            environment...
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/generate_upstream_record.drawio.png"
          />
          <p>
            ...which creates a record of two things: 1) all of the requests sent
            to the “store service&quot; during a specific time and 2) the
            &quot;store service&quot;’s corresponding responses for each of
            those requests.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/generate_upstream_script.drawio.png"
          />
          <p>
            The recorded requests from <strong>production</strong> are issued to
            the service under test in the <strong>testing</strong> environment.
            The responses are then compared with recorded responses from
            <strong>production</strong>.
          </p>
          <img class="lazy" data-src="images/diagrams/run_tests.drawio.png" />
          <p>
            Using traffic replay, the developer does not have to predict what
            production traffic looks like and how exactly the service under test
            should respond to that traffic. This makes traffic replay a good fit
            for creating regression tests when production traffic is complex and
            unpredictable.
          </p>
          <h4 id="section-2-3-2">
            2.3.2 Making Downstream Dependencies Available using Service
            Virtualization
          </h4>
          <p>
            “Service virtualization” also makes use of recorded production
            traffic, except this time it is traffic “downstream” of the
            microservice to be updated. That is, it records all outgoing
            requests from the “store service” in production and the
            corresponding responses from downstream dependencies.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/generate_downstream_record.drawio.png"
          />
          <p>
            That data is then used to “virtualize” each dependency.
            Virtualization means that a process is started in the testing
            environment for each downstream dependency. When that process
            receives an HTTP request, it searches the recorded downstream
            production traffic for an identical request that happened in
            production. When it finds that request, it issues the corresponding
            response that the “store service” received in production.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/generate_downstream_script.drawio.png"
          />
          <p>
            Instead of spinning up an entire set of microservices in a testing
            environment, which can be a long, complicated process, the developer
            spins up a single process for each downstream dependency. This
            avoids the potential integration settings and configuration problems
            associated with spinning up many services in a testing environment.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/generate_downstream_test.drawio.png"
          />
          <p>
            Virtualizing dependencies is a good fit for testing microservices
            that have many downstream dependencies which cannot be all spun up
            in the same environment:
          </p>
          <ul>
            <li>
              It makes it possible to run tests on microservices with
              third-party dependencies. The responses for a given request will
              be the same each time the test is run.
            </li>
            <li>
              There’s no need to rely on that third-party’s rate limit or
              availability.
            </li>
            <li>
              It makes the tests repeatable. The data for each test will be the
              same each time the test is run.
            </li>
          </ul>
          <h2 id="section-3">3. Why We Built Guardrail</h2>
          <p>
            Developers wanting to use traffic replay in their microservice
            testing strategy can either purchase an enterprise product or build
            their own solution using a collection of open-source tools.
            Currently, one of the leading enterprise options is Speedscale.
            Speedscale combines upstream and downstream traffic replay in a
            single solution and provides additional features such as load
            testing and chaos engineering.
          </p>
          <p>
            If a company didn’t want to pay for a full-featured solution like
            Speedscale, they could create one themselves using a combination of
            already existing open-source tools. GoReplay or StormForge’s “VHS”
            can be used to replay upstream traffic, and WireMock, Mountebank, or
            Nock can be used to virtualize services.
          </p>
          <p>
            While many open-source tools are well built and well supported,
            fully isolating a microservice with basic traffic replay
            functionality requires coordinating multiple such tools so that they
            perform cohesively across multiple environments;
            <strong>this is not a trivial task</strong>. For example, upstream
            and downstream traffic recording must be started and stopped
            simultaneously for traffic replay and service virtualization to work
            in tandem. In addition, current open-source traffic replay tools do
            not compare recorded responses with test responses, meaning
            developers would need yet another tool to “diff” the two responses
            to detect deviations.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/comparison-woguardrail.drawio.png"
          />
          <p>
            If a small team maintains an application, they may not need the
            advanced features of an enterprise traffic replay tool, and they may
            not have the time to develop their own. This is the use case for
            Guardrail. It is not as feature-rich as an enterprise product like
            Speedscale, but it is easier to deploy than the DIY options, making
            it a good fit for small teams to validate the production readiness
            of the newly upgraded services.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/solution-comparison.drawio.png"
          />
          <h2 id="section-4">4. What is Guardrail</h2>
          <p>
            Guardrail is an open-source tool that generates regression tests for
            microservices using recorded production traffic. It combines traffic
            replay and service virtualization to test a microservice in
            isolation.
          </p>
          <p>There are three core functionalities to Guardrail:</p>
          <ol>
            <li>
              <strong>Record</strong> traffic in the production environment
              <img
                class="lazy"
                data-src="images/diagrams/orchestrate_recording.drawio.png"
              />
            </li>
            <li><strong>Replay</strong> traffic in the testing environment</li>
            <img
              class="lazy"
              data-src="images/diagrams/orchestrate_replay.drawio.png"
            />
            <li>
              <strong>Report</strong> the results from the testing environment
              <img class="lazy" data-src="images/diagrams/ui.png" />
              <img class="lazy" data-src="images/diagrams/ui.png" />
            </li>
          </ol>
          <h2 id="section-5">5. Using Guardrail</h2>
          <p>
            Let’s walk through a typical workflow for a developer using
            Guardrail.
          </p>
          <h3 id="section-5-1">5.1 Record</h3>
          <p>
            The first step is to record traffic upstream and downstream of the
            microservice in production that we are working on changing.
          </p>
          <h4 id="section-5-1-1">
            5.1.1 Verify Application Meets Requirements
          </h4>
          <p>
            There are a few requirements an architecture must meet before
            Guardrail can be deployed.
          </p>
          <p>
            First, network traffic between microservices must be unencrypted.
            This scenario typically will involve a firewall and a gateway that
            separates the private and the public internet. Nginx as an API
            gateway with TrueCrypt is a basic example of this scenario. It is
            possible to use Guardrail with encrypted traffic, but the developer
            must add their own TLS termination proxy.
          </p>
          <p>
            Secondly, the application must use the “correlation ID” pattern to
            trace requests (Microsoft and contributors). A “correlation ID” is a
            unique HTTP header value attached to a request when it passes into
            an application’s private network.
          </p>
          <p>
            Thirdly, the use case is limited to non-persisted stateless services
            that are purely for data transformation.
          </p>
          <p>
            Finally, it must match the architecture&#39;s communication pattern
            and protocol. Guardrail can work with architectures with
            (synchronous) HTTP communication patterns using a combination of
            REST and JSON. It doesn&#39;t work with asynchronous communication
            patterns that use HTTP Polling or message queues.
          </p>
          <h4 id="section-5-1-2">
            5.1.2 Installation in a Production Environment
          </h4>
          <p>
            Install <a href="https://github.com/buger/goreplay">GoReplay</a>,
            <a href="https://github.com/bbyars/mountebank">Mountebank</a>, and
            <a href="https://github.com/Guardrail-service-testing/guardrail"
              >Guardrail</a
            >
            on the production machine of the microservice you will eventually be
            changing.
          </p>
          <h4 id="section-5-1-3">
            5.1.3 Change URLs of Downstream Dependencies
          </h4>
          <p>
            Traffic between the microservice and its downstream dependencies is
            recorded using a proxy, so the URLs the microservice uses to address
            those dependencies must be changed to the URLs of the proxies.
          </p>
          <img class="lazy" data-src="images/diagrams/redirect.drawio.png" />
          <p>
            The developer changes the URLs by declaring a list of downstream
            dependencies and then running the command<code>guardrail init</code
            >.
          </p>
          <img class="lazy" data-src="images/diagrams/init.png" />
          <h4 id="section-5-1-4">5.1.4 Start Recording</h4>
          <p>
            The developer then uses the <code>guardrail record</code> command.
          </p>
          <img class="lazy" data-src="images/diagrams/record.png" />
          <p>
            From that point on, GoReplay is recording upstream traffic and
            Mountebank is recording downstream traffic. Traffic is recorded to
            the production host’s file system.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/recording_output.drawio.png"
          />
          <h4 id="section-5-1-5">5.1.5 Stop Recording</h4>
          <p>
            The developer can then stop upstream and downstream traffic
            recording by quitting Guardrail (^C). Finally, the URLs addressing
            the downstream recording proxies should be reverted to the URLs that
            point directly towards the downstream dependencies.
          </p>
          <h3 id="section-5-2">5.2 Replay</h3>
          <h4 id="section-5-2-1">5.2.1 Setup the Testing Environment</h4>
          <p>
            The developer then downloads GoReplay, Mountebank, and Guardrail to
            whatever machine they will run their tests on. We will assume that
            the machine is the developer’s local machine.
          </p>
          <p>
            They then spin up the updated microservice (in Aaron’s case, “store
            service 2.0”) on that machine. The microservice should be configured
            using the same URLs used by the microservice in production during
            recording. If it is configured with addresses of the actual
            dependencies, the service under test will issue requests to the real
            production dependencies..
          </p>
          <h4 id="section-5-2-2">5.2.2 Data Transfer</h4>
          <p>
            Next, the developer manually transfers the files of recorded traffic
            from the production host to the host of the machine running the
            testing environment.
          </p>
          <h4 id="section-5-2-3">5.2.3 Replay Traffic</h4>
          <p>
            The developer then runs the <code>guardrail replay</code> command in
            the testing host. This starts up the Mountebank virtualized services
            using the data collected from production and then replays the
            upstream requests against the service under test using GoReplay. It
            also starts up a component of Guardrail called the “Reporting
            Service,” which becomes relevant in the next section.
          </p>
          <img class="lazy" data-src="images/diagrams/all_tests.drawio.png" />
          <p>
            The same traffic recording can be replayed multiple times, allowing
            developers to iterate on the service under test without having to
            re-record traffic.
          </p>
          <h3 id="section-5-3">5.3 Report Results</h3>
          <h4 id="3-1-calculate-and-view-results">
            3.1 Calculate and View Results
          </h4>
          <p>
            In addition to storing traffic data in a database, the Reporting
            Service calculates the results of a replay session and serves the
            results to the Guardrail user interface. Results are a comparison of
            the actual and expected HTTP response status, headers, and JSON
            body.
          </p>
          <p>
            Once a replay session finishes, the developer can access the user
            interface from a web browser of the machine running the test
            environment. Requests that had different responses from what was
            recorded in production are listed, along with the expected and
            actual responses.
          </p>
          <img class="lazy" data-src="images/diagrams/ui.png" />
          <h2 id="section-6">6. Design Decisions</h2>
          <p>
            There are many ways to build a tool like Guardrail. Here are some of
            the “forks in the road” we encountered when building Guardrail and
            the reasoning behind the directions we chose.
          </p>
          <h3 id="section-6-1">
            6.1 Inserting Upstream Recording Instrumentation
          </h3>
          <p>
            One of the first significant design decisions we encountered was
            which tool to use for recording traffic. There are two leading
            approaches to recording traffic of a single microservice in the
            production environment: proxy and packet capture (PCAP).
          </p>
          <h4 id="section-6-1-1">6.1.1 HTTP Proxy</h4>
          <p>
            The aptly named proxy approach requires starting an
            application-level proxy server to sit in front of your microservice.
            The proxy server handles recording incoming and outgoing traffic
            that passes through it. All incoming traffic must be directed to the
            proxy server instead of your microservice. Building it is relatively
            simple, although redirecting traffic may not be.
          </p>
          <p>
            With this approach, consumers of the “store service” must connect to
            the proxy instead. A server listens on a specific port, and the
            combination of that binding port and the host IP makes the server
            addressable in the network. A proxy will have a new address, and
            clients must connect to it temporarily.
          </p>
          <p>
            If there is only one upstream client, that might be ok. But we
            don&#39;t know how many client services connect to a microservice.
          </p>
          <p>
            One could potentially address this complication with DNS. Changing
            DNS records can redirect traffic, and a local DNS resolver can
            divert traffic to a different host. It won&#39;t point to the same
            host with a different port, though it can still work.
          </p>
          <p>
            However, managing DNS records is problematic without integrating
            them into an orchestration system or leaving it up to the user to
            manage. Without it, there are very few assumptions we can make about
            the underlying infrastructure.
          </p>
          <p>Let’s consider the other approach.</p>
          <h4 id="section-6-1-2">6.1.2 UNIX PCAP</h4>
          <p>
            Alternatively, packet capture doesn&#39;t require any additional
            infrastructure or redirection of traffic. You run a packet capture
            process on your production server, and it watches a specific network
            interface and records traffic from incoming connections.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/upstream_packet_capture.drawio.png"
          />
          <p>
            Unlike application proxies, packet capture does not have direct
            access to HTTP messages. IP packets are duplicated on the host&#39;s
            network interface. But, IP is a lower-level protocol, so an extra
            step has to bring the IP payload to the application layer. This step
            assembles the packet payloads to get the TCP segments and then
            eventually to HTTP messages.
          </p>
          <p>
            There are open-source tools that take advantage of packet capture
            and bring the IP payload to the application layer - GoReplay being
            one of them. GoReplay has the added advantage over other PCAP tools
            because it can replay recorded traffic.
          </p>
          <p>
            Because application proxies are more disruptive, we decided to go
            with a packet capture approach with GoReplay.
          </p>
          <h3 id="section-6-2">
            6.2 Inserting Downstream Recording Instrumentation
          </h3>
          <p>
            There are different considerations when it comes to deciding how to
            record requests between the microservice in production and its
            downstream dependency services. We have to use a proxy because we
            can&#39;t use packet capture.
          </p>
          <p>
            Packet capture is no longer an option for recording downstream
            dependencies because it works only on a specified port. It&#39;s
            suitable for published server ports that are static but not good for
            HTTP clients. As an HTTP client, a random non-standard ephemeral
            port is used to establish a connection, and this is why we have to
            rely on a proxy to record downstream traffic.
          </p>
          <p>
            Without packet capture, using proxies to record traffic becomes the
            most promising approach, which begs the question: how do we direct
            traffic away from the dependency to the proxy?
          </p>
          <p>One of these three will need to be changed:</p>
          <ul>
            <li>The HTTP client library</li>
            <li>The local DNS records of the dependencies</li>
            <li>The configuration of the dependency URLs</li>
          </ul>
          <p>
            Changing the library used to issue HTTP requests would require code
            modification for the microservice. The open-source tool, Nock, can
            be used to accomplish this in microservices written in JavaScript,
            and VCR can be used in microservices written in Ruby. The main
            tradeoff of this approach is that we&#39;d be restricted to testing
            microservices written in languages that those tools can support, and
            we would require additional changes to their code. We would rather
            use a solution that occurs out-of-process.
          </p>
          <p>
            We considered redirecting traffic through DNS, but this needs to be
            managed externally, which may be unwieldy. Redirecting traffic this
            way will require changing the DNS record for a downstream dependency
            to point to a different IP. Each recording proxy for a virtual
            dependency may have to be hosted on different IPs because DNS can
            only redirect to a different IP but not redirect to a different
            port.
          </p>
          <p>
            In addition, the process of modifying DNS records differs greatly
            according to how an application is deployed. Some applications
            deployed on AWS are able to interact with their DNS using Route 55,
            some using Kubernetes can interact with a cluster’s DNS using
            CoreDNS, while some do not have an easy way to modify DNS records at
            all. We opted for a solution that is not limited to a particular
            deployment strategy.
          </p>
          <p>
            Changes to the configuration file are easier to manage compared to
            the first two methods. Unlike the HTTP library, this will only be a
            configuration change and not a code change. But of course, there is
            an added user step and we need to assume the convention of
            extracting URLs for downstream dependencies into configuration files
            (Wiggins).
          </p>
          <p>
            To use Guardrail then, the user will have to change the service
            configuration manually for the recording duration. Changing the URL
            in the configuration redirects traffic, and reverting the changes
            redirects traffic back.
          </p>
          <img class="lazy" data-src="images/diagrams/redirect.drawio.png" />
          <h3 id="section-6-3">
            6.3 Coordinating Upstream and Downstream Recording
          </h3>
          <p>
            Being able to record upstream and downstream traffic is not enough
            to create meaningful tests. The data collection procedure needs
            coordination. Specifically, the downstream recording must start
            before upstream is started and upstream recording must stop before
            downstream is stopped. Otherwise, the virtualized dependencies will
            be issued requests that they don’t have the data to respond to and
            the tests will fail.
          </p>
          <p>
            Our solution takes advantage of the fact that GoReplay and
            Mountebank both run on the same host as the microservice in
            production. Because GoReplay and Mountebank run on the same host, we
            are able to orchestrate their behavior by creating scripts executed
            using the host’s CLI. This saves the developer from having to know
            how to coordinate upstream and downstream recording and replay,
            simplifying the process to one “record” command in production and
            one “replay” command in testing.
          </p>
          <h3 id="section-6-4">
            6.3 Correlating Upstream Traffic with Downstream Traffic
          </h3>
          <p>
            Virtualized services do not know how to disambiguate between
            multiple requests with the same signature.
          </p>
          <p>
            When replaying traffic against the service under test, one expects
            some responses to be different from the responses recorded in
            production. Guardrail is built to catch those differences. However,
            we found that some responses occurring during replay contained data
            that pertained to the wrong request.
          </p>
          <p>Expected test results</p>
          <img
            class="lazy"
            alt="Expected test results"
            data-src="images/diagrams/no_mismatch.drawio.png"
          />
          <p>Actual test results</p>
          <img
            class="lazy"
            alt="Actual test results"
            data-src="images/diagrams/mismatch_problem.drawio.png"
          />
          <p>
            This traffic mismatch only occurs with identical requests that
            happened concurrently in production. If the recorded requests in
            production have different paths and are spaced out over time, there
            is no traffic mismatch.
          </p>
          <p>
            This error is a result of how Mountebank works when replaying
            traffic. When Mountebank is in “replay” mode, it pattern-matches an
            incoming request to a stub. The matching stub then generates an HTTP
            response from a list of one or more recorded responses.
          </p>
          <p>
            If there is more than one recorded response for a given request,
            Mountebank, on each subsequent matching request, will iterate
            through its list of recorded responses, starting over once it
            reaches the end. If a dependency in production responds to requests
            in the order it receives those requests, then Mountebank will
            respond to every request correctly in “replay.”However, dependencies
            are not guaranteed to respond to requests in the order that they
            receive them.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/mismatch_explained.drawio.png"
          />
          <p>
            This is a problem when Mountebank relies on the order the responses
            occurred in production when deciding how to respond to multiple
            identical requests. To prevent this, we need a way to tie together
            related upstream and downstream traffic so that Guardrail recognizes
            them as a single thread.
          </p>
          <p>
            Our solution was to require applications using Guardrail to use
            correlation IDs in their back-end traffic. An “X-Correlation-ID”
            HTTP header is a unique request identifier for every incoming user
            request. Devices on the request path add this header as early as
            possible, and they pass them along the entire request/response
            cycle, both upstream and downstream. API gateways or web servers
            such as NGINX and Apache support this feature, though the naming
            varies slightly.
          </p>
          <p>
            When recorded traffic has a correlation ID pattern, there no longer
            is such a thing as “identical requests.” Each request issued to a
            dependency has exactly one associated response, so there is no
            possibility for out-of-order production traffic to lead to
            mismatched traffic in testing.
          </p>
          <img
            class="lazy"
            data-src="images/diagrams/mismatch_solution.drawio.png"
          />
          <h3 id="section-6-5">6.4 Collecting Data in One Place</h3>
          <p>
            There are three critical pieces of information when it comes to
            testing microservices using production traffic:
          </p>
          <ol>
            <li>The recorded user request from the production environment</li>
            <li>
              The recorded response to the user from the production environment
            </li>
            <li>
              The replayed response from staging (what our service under test
              responds with when we replay the recorded user request).
            </li>
          </ol>
          <img class="lazy" data-src="images/diagrams/pieces.drawio.png" />
          <p>
            For Guardrail’s use case, none of this data is very useful in
            isolation. We need all three of these pieces combined into a single
            unit that we can evaluate. We refer to this three-part unit as a
            “triplet.”
          </p>
          <img class="lazy" data-src="images/diagrams/triplet.drawio.png" />
          <p>
            Conveniently, GoReplay outputs a file containing this information
            after replaying recorded traffic. However, it does not group the
            information into meaningful triplets. Production requests,
            production responses, and replayed responses are all mixed together
            in the file.
          </p>
          <p>
            If we were to use that file, we would need to read each GoReplay
            traffic file into memory, parse the data, and group the entries into
            triplets using their correlation ID.
          </p>
          <img class="lazy" data-src="images/diagrams/mess.drawio.png" />
          <p>
            The alternative was to use GoReplay’s middleware to process
            components as they arrive and forward them to Guardrail’s reporting
            service, where they’re assembled into triplets and compared to
            generate a final report with test results.
          </p>
          <img class="lazy" data-src="images/diagrams/mongodb.drawio.png" />
          <p>
            Comparing these two options, processing components using middleware
            is computationally less intensive than parsing files and matching
            components based on correlation ID.
          </p>
          <h2 id="section-7">7. Future Work</h2>
          <p>
            We designed Guardrail to solve the current use case we envisioned.
            However, we did notice some areas we can improve on for future
            iterations.
          </p>
          <h3 id="section-7-1">7.1 Encryption</h3>
          <p>
            Encryption was not the core problem on the initial implementation,
            but it&#39;s among the more critical parts to address. The industry
            broadly recognizes that encryption of user data should happen by
            default.
          </p>
          <p>
            We want to add a solution to transfer data across environments
            securely, encrypt it at rest, and provide access control for
            developers and operators.
          </p>
          <h3 id="section-7-2">7.2 Diffing Outbound Requests</h3>
          <p>
            Currently, the regression simulations are only on the API consumer
            side. We want to add the ability to show unexpected downstream
            requests associated with an upstream request.
          </p>
          <h3 id="section-7-3">7.3 Toggle Simulating Response Times</h3>
          <p>
            We would like to add the ability to toggle the ability to simulate
            the downstream response times.
          </p>
          <p>
            There are tradeoffs between test execution time and seeing the
            response time comparison of test and record traffic. If you emulate
            response time, the test will run longer, but you can&#39;t see the
            comparison and vice versa.
          </p>
          <h3 id="section-7-4">7.4 Filtering Requests</h3>
          <p>
            Another thing we can easily add is the ability to filter requests.
            Guardrail is aimed only at non-persisted microservices, but
            potentially, this filter could allow testing on a particular
            database snapshot. Using Guardrail in microservices with databases
            needs further investigation.
          </p>
          <h3 id="section-7-5">7.5 Comparison Middleware</h3>
          <p>
            Guardrail checks the payload&#39;s deeply nested body by comparing
            attributes in the JSON payload. This comparison may not fit all use
            cases.
          </p>
          <p>
            If the payload returned by the service under test includes a
            timestamp, there would be a different result. The test case would
            fail even though the body produced is correct except for the
            timestamp having another value.
          </p>
          <p>
            There&#39;s no way of knowing in advance if and where the timestamp
            would be included. If we do, we can skip it from the comparison.
            There&#39;s no universal format for timestamps that we can filter
            out.
          </p>
          <p>
            Future work may consider using the Levenstein distance algorithm to
            compare how close the two responses are from each other (Cuelogic).
            Alternatively, we could give the testers the ability to define which
            response is considered a fail or a pass through an optional
            middleware.
          </p>
          <h2 id="section-8">8. Conclusion</h2>
          <p>
            Guardrail is a tool designed to test stateless non-persisted
            microservices limited to data transformation tasks using synchronous
            HTTP communication. With that use case, we can test using only
            traffic replay and service virtualization.
          </p>
          <p>
            We identified the main challenges of testing a microservice, and
            they center around creating test data and recreating the
            environment. Creating test data needs a good understanding of the
            API, and those tests need to be separately maintained. Furthermore,
            the environment needs to be emulated with service virtualization.
          </p>
          <p>
            We believe we&#39;ve built Guardrail into an open-source tool that
            is well-positioned to deliver significant testing value to
            engineering teams in charge of budding microservices. The core value
            provided by Guardrail is that it abstracts and automates much of the
            complexity and tedium involved with implementing and maintaining
            good, accurate regression tests for microservices. We hope that
            Guardrail can help you prevent your deployed changes from being in
            the 15% that cause a production incident.
          </p>
          <h2 id="section-9">9. Bibliography</h2>
          <ol>
            <li>
              <p>
                Cuelogic. “The Levenshtein Algorithm.”
                <em
                  ><a href="https://www.cuelogic.com/blog"
                    >https://www.cuelogic.com/blog</a
                  ></em
                >, 2017,
                <a
                  href="https://www.cuelogic.com/blog/the-levenshtein-algorithm"
                  >https://www.cuelogic.com/blog/the-levenshtein-algorithm</a
                >. Accessed 14 September 2021.
              </p>
            </li>
            <li>
              <p>
                DevOps Research and Assessment. “2019 State of DevOps Report.”
                <em
                  ><a
                    href="https://www.devops-research.com/research.html#reports"
                    >https://www.devops-research.com/research.html#reports</a
                  ></em
                >, 2019,
                <a
                  href="https://services.google.com/fh/files/misc/state-of-devops-2019.pdf"
                  >https://services.google.com/fh/files/misc/state-of-devops-2019.pdf</a
                >. Accessed 14 September 2021.
              </p>
            </li>
            <li>
              <p>
                Glover, Andy, and Katharina Probst. “Tips for High
                Availability.”
                <em
                  ><a href="https://netflixtechblog.medium.com/"
                    >https://netflixtechblog.medium.com/</a
                  ></em
                >, 2018,
                <a
                  href="https://netflixtechblog.medium.com/tips-for-high-availability-be0472f2599c"
                  >https://netflixtechblog.medium.com/tips-for-high-availability-be0472f2599c</a
                >. Accessed 14 September 2021.
              </p>
            </li>
            <li>
              <p>
                Google, Inc. “Site Reliability Engineering.”
                <em
                  ><a href="https://sre.google/books/"
                    >https://sre.google/books/</a
                  ></em
                >, O’Reilly, 2018,
                <a href="https://sre.google/workbook/canarying-releases/"
                  >https://sre.google/workbook/canarying-releases/</a
                >. Accessed 14 September 2021.
              </p>
            </li>
            <li>
              <p>
                Microsoft and contributors. “CSE Code-With Engineering
                Playbook.”
                <em
                  ><a
                    href="https://microsoft.github.io/code-with-engineering-playbook/"
                    >https://microsoft.github.io/code-with-engineering-playbook/</a
                  ></em
                >,
                <a
                  href="https://microsoft.github.io/code-with-engineering-playbook/observability/correlation-id/"
                  >https://microsoft.github.io/code-with-engineering-playbook/observability/correlation-id/</a
                >. Accessed 14 September 2021.
              </p>
            </li>
            <li>
              <p>
                Toby Clemenson. “Testing Strategies in a Microservice
                Architecture.”
                <em
                  ><a href="https://martinfowler.com/microservices/"
                    >https://martinfowler.com/microservices/</a
                  ></em
                >, 2014,
                <a
                  href="https://martinfowler.com/articles/microservice-testing/"
                  >https://martinfowler.com/articles/microservice-testing/</a
                >. Accessed 14 September 2021.
              </p>
            </li>
            <li>
              <p>
                Richardson, Chris. <em>Microservices Patterns</em>. Manning
                Publications, 2019.
              </p>
            </li>
            <li>
              <p>
                Sridharan, Cindy. “Testing in Production: the hard parts.”
                <em
                  ><a href="https://copyconstruct.medium.com/"
                    >https://copyconstruct.medium.com/</a
                  ></em
                >, 2019,
                <a
                  href="https://copyconstruct.medium.com/testing-in-production-the-hard-parts-3f06cefaf592"
                  >https://copyconstruct.medium.com/testing-in-production-the-hard-parts-3f06cefaf592</a
                >. Accessed 14 September 2021.
              </p>
            </li>
            <li>
              <p>
                Wiggins, Adam. “The Twelve-Factor App.”
                <em
                  ><a href="https://12factor.net/">https://12factor.net/</a></em
                >,
                <a href="https://12factor.net/config"
                  >https://12factor.net/config</a
                >. Accessed 14 September 2021.
              </p>
            </li>
          </ol>
        </div>
      </div>
    </div>

    <div id="presentation" class="main-section">
      <div class="bg-gray">
        <h2>Presentation</h2>
        <iframe
          src="https://www.youtube-nocookie.com/embed/OT9qiQZ9SKk"
          title="YouTube video player"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
      </div>
    </div>

    <div id="our-team" class="main-section">
      <div>
        <div>
          <div>
            <h2>Meet our team</h2>
            <p class="text-xl text-gray-300">
              We are currently looking for opportunities. If you liked what you
              saw and want to talk more, please reach out!
            </p>
          </div>
          <ul class="people">
            <li class="profile">
              <img
                class="mx-auto h-40 w-40 rounded-full xl:w-56 xl:h-56 lazy"
                data-src="images/team/jd.jpg"
                alt=""
              />
              <div>
                <div>
                  <h3>James Duot</h3>
                  <p>Los Angeles, CA, USA</p>
                </div>

                <ul class="social">
                  <li>
                    <a href="mailto:duot.jim@gmail.com" target="_blank"
                      ><i class="fas fa-envelope"></i
                    ></a>
                  </li>
                  <li>
                    <a
                      href="https://www.linkedin.com/in/james-duot-5241a4100/"
                      target="_blank"
                      ><i class="fab fa-linkedin"></i
                    ></a>
                  </li>
                  <li>
                    <a href="https://github.com/duot" target="_blank"
                      ><i class="fab fa-github"></i
                    ></a>
                  </li>
                  <li>
                    <a href="https://duot.github.io/" target="_blank"
                      ><i class="fas fa-globe"></i
                    ></a>
                  </li>
                </ul>
              </div>
            </li>

            <li class="profile">
              <img
                class="mx-auto h-40 w-40 rounded-full xl:w-56 xl:h-56 lazy"
                data-src="images/team/jt.jpg"
                alt=""
              />
              <div>
                <div>
                  <h3>Jordan Thomas</h3>
                  <p>Durham, NC, USA</p>
                </div>

                <ul class="social">
                  <li>
                    <a href="mailto:jordan.thomas789@gmail.com" target="_blank"
                      ><i class="fas fa-envelope"></i
                    ></a>
                  </li>
                  <li>
                    <a
                      href="https://www.linkedin.com/in/jordan-thomas-21b9413a/"
                      target="_blank"
                      ><i class="fab fa-linkedin"></i
                    ></a>
                  </li>
                  <li>
                    <a href="https://github.com/jordan-th" target="_blank"
                      ><i class="fab fa-github"></i
                    ></a>
                  </li>
                  <li>
                    <a href="https://TODO.github.io/" target="_blank"
                      ><i class="fas fa-globe"></i
                    ></a>
                  </li>
                </ul>
              </div>
            </li>

            <li class="profile">
              <img
                class="mx-auto h-40 w-40 rounded-full xl:w-56 xl:h-56 lazy"
                data-src="images/team/tc.jpg"
                alt=""
              />
              <div>
                <div>
                  <h3>Tim Cummings</h3>
                  <p>Nashville, TN, USA</p>
                </div>

                <ul class="social">
                  <li>
                    <a href="mailto:timjc86@gmail.com" target="_blank"
                      ><i class="fas fa-envelope"></i
                    ></a>
                  </li>
                  <li>
                    <a
                      href="https://www.linkedin.com/in/timothy-cummings-3a246b160/"
                      target="_blank"
                      ><i class="fab fa-linkedin"></i
                    ></a>
                  </li>
                  <li>
                    <a href="https://github.com/TimCummings" target="_blank"
                      ><i class="fab fa-github"></i
                    ></a>
                  </li>
                  <li>
                    <a href="https://TODO.github.io/" target="_blank"
                      ><i class="fas fa-globe"></i
                    ></a>
                  </li>
                </ul>
              </div>
            </li>
          </ul>
        </div>
      </div>
    </div>

    <script src="javascripts/script.js"></script>
  </body>
</html>
